# Feature Specification: 修复 Batch LLM 调用默认配置

**Feature Branch**: `007-fix-batch-llm-defaults`
**Created**: 2026-02-14
**Status**: Draft
**Input**: 修复 batch spec 生成中的三个关键问题：默认模型太慢、系统提示词重复注入、超时时间不合理

## User Scenarios & Testing *(mandatory)*

### User Story 1 - 批量生成不再因超时反复失败 (Priority: P1)

作为使用 `reverse-spec batch` 的开发者，我希望批量生成过程中每个模块都能在合理时间内完成 LLM 调用，而不是因为默认模型响应太慢导致超时失败、重试三次后报错，浪费十几分钟却无任何产出。

**Why this priority**: 这是最核心的可用性问题。当前默认配置下，即使只有 2 个文件的小模块，LLM 调用也会因超时而失败，导致整个 batch 流程几乎不可用。修复后才能让用户正常使用批量生成功能。

**Independent Test**: 在包含多个模块的真实项目上运行 `reverse-spec batch`，验证所有模块（包括小模块和大模块）都能成功完成 LLM 调用，不再因超时反复失败。

**Acceptance Scenarios**:

1. **Given** 用户在一个包含多个模块的项目中运行 batch 命令，**When** 使用默认配置（未设置环境变量覆盖模型），**Then** 每个模块的 LLM 调用都应在合理时间内返回结果，不出现超时失败
2. **Given** 用户通过环境变量显式指定了慢速模型，**When** 运行 batch 命令，**Then** 系统应使用与该模型匹配的更长超时时间，避免因超时截断有效响应
3. **Given** 一个模块因网络或服务端原因真正超时，**When** 重试仍然失败，**Then** 系统应快速判定失败并跳过该模块，而非耗费所有重试次数后才报错

---

### User Story 2 - 消除冗余提示词浪费 (Priority: P2)

作为使用 reverse-spec 的开发者，我希望发送给 LLM 的请求中不包含重复的系统提示词，避免浪费 token 配额和增加不必要的延迟。

**Why this priority**: 系统提示词被重复注入意味着每次 LLM 调用浪费约 2000 token，在 batch 处理几十个模块时累计浪费显著。同时冗余内容也会增加 LLM 处理时间，间接加剧超时风险。

**Independent Test**: 对单个模块运行 spec 生成，检查实际发送给 LLM 的 prompt 中系统提示词只出现一次。

**Acceptance Scenarios**:

1. **Given** 系统组装完上下文后准备调用 LLM，**When** 通过任何调用路径（直接调用或代理调用）发送请求，**Then** 系统提示词在最终发送内容中只出现一次
2. **Given** 用户使用不同的认证方式（直接认证或代理认证），**When** 生成 spec，**Then** 两种路径产出的 spec 质量和内容一致，不因提示词处理差异产生结果差异

---

### User Story 3 - 用户可覆盖默认模型 (Priority: P3)

作为高级用户，我希望能通过环境变量或配置选择不同的 LLM 模型，系统能根据所选模型自动调整超时等参数，无需手动配置每个细节。

**Why this priority**: 不同用户对生成质量和速度有不同偏好。有些用户愿意等待更长时间以获得高质量输出，有些用户更看重速度。允许用户灵活选择模型并自动适配参数是良好的用户体验。

**Independent Test**: 通过环境变量指定不同模型后运行 spec 生成，验证系统使用了对应模型并自动匹配合理的超时时间。

**Acceptance Scenarios**:

1. **Given** 用户通过环境变量指定了一个较慢的模型，**When** 运行 spec 生成，**Then** 系统自动使用更长的超时时间
2. **Given** 用户未设置任何环境变量，**When** 运行 batch 命令，**Then** 系统使用性价比最优的默认模型，确保 batch 场景的流畅性
3. **Given** 用户通过环境变量指定了一个快速模型，**When** 运行 spec 生成，**Then** 系统使用标准超时时间，不会不必要地等待

---

### Edge Cases

- 当用户设置的环境变量值不是已知模型名称时，系统应使用该值作为模型名并采用保守的超时策略
- 当 LLM 在超时边界返回不完整响应时，系统应视为失败而非使用残缺内容
- 当同一个 batch 运行中混合存在大模块和小模块时，超时策略应对所有模块一致（由模型决定，而非由模块大小决定）

## Requirements *(mandatory)*

### Functional Requirements

- **FR-001**: 系统 MUST 在 batch 场景下使用速度更快的默认模型，确保常规模块的 LLM 调用不会超时
- **FR-002**: 系统 MUST 确保发送给 LLM 的请求中系统提示词只出现一次，无论使用哪种调用路径
- **FR-003**: 系统 MUST 根据所选模型自动确定合理的超时时间，慢速模型对应更长超时、快速模型对应标准超时
- **FR-004**: 系统 MUST 保留通过环境变量覆盖默认模型的能力，用户显式设置的模型优先级高于系统默认值
- **FR-005**: 系统 MUST 在单文件 generate 命令下也消除提示词重复问题，保持与 batch 一致的行为

### Key Entities

- **LLM 调用配置**: 包含模型标识、超时时间、最大重试次数等参数，决定每次 LLM 调用的行为
- **系统提示词**: 指导 LLM 生成 spec 的指令文本，在上下文组装阶段注入，应在整个调用链路中只出现一次

## Success Criteria *(mandatory)*

### Measurable Outcomes

- **SC-001**: 在包含 20 个以上模块的真实项目上运行 batch 命令，使用默认配置时模块 LLM 调用成功率达到 95% 以上（之前为 0%）
- **SC-002**: 每次 LLM 调用的 token 消耗减少至少 1500 token（消除重复提示词带来的浪费）
- **SC-003**: 默认配置下单个模块的 LLM 调用平均完成时间不超过 120 秒
- **SC-004**: 用户通过环境变量切换模型后，系统自动适配超时参数，无需额外配置

## Assumptions

- 当前默认模型（较慢的模型）在 batch 场景下响应时间通常超过 120 秒，不适合作为批量处理的默认选择
- 较快的模型在 spec 生成任务上的输出质量足以满足大多数用户需求
- 用户可以接受在需要最高质量输出时通过环境变量手动指定慢速模型
- 系统提示词的 token 占用约为 2000 token，重复注入导致的浪费在 batch 处理多个模块时累计显著

## Scope Boundaries

### In Scope

- 更换 batch 场景的默认 LLM 模型
- 消除两条 LLM 调用路径中的系统提示词重复注入
- 实现基于模型的动态超时策略
- 保持环境变量覆盖机制

### Out of Scope

- 修改 spec 生成的系统提示词内容本身
- 添加新的 LLM 调用路径或认证方式
- 修改 batch 编排逻辑（拓扑排序、检查点等）
- 提供 CLI 参数级别的模型选择（仅通过环境变量）
